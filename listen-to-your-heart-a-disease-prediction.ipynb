{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1471761,"sourceType":"datasetVersion","datasetId":863468},{"sourceId":5530058,"sourceType":"datasetVersion","datasetId":2425008}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === 'Secret Sauce' CSS & Config File ===\n# --- Libraries for 'Secret Sauce' ---\nimport sys\nfrom IPython.display import display, HTML, Javascript\n\n# ---'Secret Sauce' Function ---\ndef apply_config_file(kaggle_path, local_path, project_year, project_name):\n    sys.path.append(f\"{kaggle_path}/config_file/{project_year}/{project_name}/\")\n    sys.path.append(f\"{local_path}/config_file/{project_year}/{project_name}\")\n\ndef apply_css_file(kaggle_path, local_path, project_year, css_file_name):\n    try:\n        return HTML(\"<style>\"+ open(f\"{kaggle_path}/css/{project_year}/{css_file_name}.css\", \"r\").read()+ \"</style>\")\n    except:\n        return HTML(\"<style>\"+ open(f\"{local_path}/css/{project_year}/{css_file_name}.css\", \"r\").read()+ \"</style>\")\n\n# --- 'Secret Sauce' Variables ---\n\"\"\" Change Variables ~! HERE !~ \"\"\"\nproject_name = \"heart_disease\"\nproject_year = \"2023\"\ncss_file_name = \"css_\" + project_name\nkaggle_path = \"/kaggle/input/caesarmario\"\nlocal_path = \"config_kaggle\"\n\n# --- Apply Function & Import Config File ---\napply_config_file(kaggle_path, local_path, project_year, project_name)\nfrom config_file import *\napply_css_file(kaggle_path, local_path, project_year, css_file_name)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-28T08:02:05.437318Z","iopub.execute_input":"2023-04-28T08:02:05.438026Z","iopub.status.idle":"2023-04-28T08:02:09.570398Z","shell.execute_reply.started":"2023-04-28T08:02:05.437958Z","shell.execute_reply":"2023-04-28T08:02:09.568869Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!--- Project Title --->\n<span class=\"dates\">IV.XX.MMXXIII | @caesarmario</span><br>\n<span class=\"title-normal\">Listen to Your Heart: </span>\n<span class=\"title-highlight\"> A Disease Prediction</span><br>\n<span class=\"subtitle\">using Various Machine Learning Models</span>\n<hr>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{}},{"cell_type":"markdown","source":"# <div class=\"header1\">1. | Introduction üëã</div>\n<center>\n    <img src=\"https://images.unsplash.com/photo-1628348070889-cb656235b4eb?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1170&q=80\" alt=\"Heart Disease\" width=\"80%\">\n</center>","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"markdown","source":"## <div class=\"header2\">ü§î Dataset Problems</div>\n<div class=\"explain-box\">\n    This dataset is taken from the <a style=\"color: #3D5A80\" href=\"https://archive.ics.uci.edu/ml/datasets/heart+disease\"><b>UCI machine learning website</b></a>. This dataset contains <b>medical information on patients and the diagnosis results</b> of whether the patient has heart disease. <mark>Machine learning models are necessary to determine whether a patient has heart disease and speed up the diagnostic process</mark> based on the medical information provided about that patient. The <b>variables that most influence</b> a patient to have heart disease will also be <b>explored more deeply</b> in this notebook.\n</div>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## <div class=\"header2\">üìå Notebook Objectives</div>\n<div class=\"explain-box\">\n    This notebook <b>aims</b> to:\n    <ul>\n        <li><mark>Perform dataset exploration</mark> using various types of data visualization.</li>\n        <li><mark>Build machine learning model</mark> that can predict patients status.</li>\n        <li><mark>Export prediction result on test data</mark> into files.</li>\n        <li><mark>Save/dump the complete machine learing pipeline</mark> for later usage.</li>\n        <li><mark>Perform prediction on new example data</mark> given and <mark>export the prediction result</mark>.</li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"markdown","source":"## <div class=\"header2\">üë®‚Äçüíª Machine Learning Model</div>\n<div class=\"explain-box\">\n    The <b>models</b> used in this notebook:\n    <ol start=\"1\">\n        <li><b>Logistic Regression</b>,</li>\n        <li><b>K-Nearest Neighbour (KNN)</b>,</li>\n        <li><b>Support Vector Machine (SVM)</b>,</li>\n        <li><b>Gaussian Naive Bayes</b>,</li>\n        <li><b>Decision Tree</b>,</li>\n        <li><b>Random Forest</b>,</li>\n        <li><b>Extra Tree Classifier</b>,</li>\n        <li><b>Gradient Boosting</b>, and</li>\n        <li><b>AdaBoost</b>.</li>\n    </ol>\n</div>","metadata":{}},{"cell_type":"markdown","source":"## <div class=\"header2\">üßæ Dataset Description</div>\n<div class=\"explain-box\">\n    The following is the <b>structure of the dataset</b>.<br>\n    \n<table style=\"font-family: Open Sans; font-weight: 300; font-size: 12px; text-align: left; padding: 8px; border-collapse: collapse; width: 100%;\">\n  <thead>\n    <tr>\n      <th style=\"font-family: Open Sans; font-weight: 900; text-align: center; font-size: 14px; background-color: #FF5C8A\">Variable Name</th>\n      <th style=\"font-family: Open Sans; font-weight: 900; text-align: center; font-size: 14px; background-color: #FF5C8A\">Description</th>\n      <th style=\"font-family: Open Sans; font-weight: 900; text-align: center; font-size: 14px; background-color: #FF5C8A\">Sample Data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n        <td><b>Age</b></td>\n        <td>Patient age <br> (in years)</td>\n        <td>63; 37; ...</td>\n    </tr>\n    <tr>\n        <td><b>Sex</b></td>\n        <td>Gender of patient<br><br>0 = male<br>1 = female</td>\n        <td>1; 0; ...</td>\n    </tr>\n    <tr>\n        <td><b>cp</b></td>\n        <td>Chest pain type<br><br>0 = typical angina<br>1 = atypical angina<br>2 = non-anginal pain<br>3 = asymptomatic</td>\n        <td>3; 1; 2; ...</td>\n    </tr>\n    <tr>\n        <td><b>trestbps</b></td>\n        <td>Resting blood pressure <br> (in mm Hg)</td>\n        <td>145; 130; ...</td>\n    </tr>\n    <tr>\n        <td><b>chol</b></td>\n        <td>Serum cholestoral <br> (in mg/dl)</td>\n        <td>233; 250; ...</td>\n    </tr>\n    <tr>\n        <td><b>fbs</b></td>\n        <td>Fasting blood sugar > 120 mg/dl<br><br>0 = false<br>1 = true</td>\n        <td>1; 0; ...</td>\n    </tr>\n    <tr>\n        <td><b>restecg</b></td>\n        <td>Resting electrocardiographic results<br><br>0 = normal<br>1 = having ST-T wave abnormality<br>2 = showing probable or definite left ventricular hypertrophy by Estes' criteria</td>\n        <td>0; 1; ...</td>\n    </tr>\n    <tr>\n        <td><b>thalach</b></td>\n        <td>Maximum heart rate achieved </td>\n        <td>150; 187; ...</td>\n    </tr>\n    <tr>\n        <td><b>exang</b></td>\n        <td>Exercise induced angina<br><br>0 = no<br>1 = yes</td>\n        <td>1; 0; ...</td>\n    </tr>\n    <tr>\n        <td><b>oldpeak</b></td>\n        <td>ST depression induced by exercise relative to rest</td>\n        <td>2.3; 3.5; ...</td>\n    </tr>\n    <tr>\n        <td><b>slope</b></td>\n        <td>The slope of the peak exercise ST segment<br><br>0 = upsloping<br>1 = flat<br>2 = downsloping</td>\n        <td>0; 2; ...</td>\n    </tr>\n    <tr>\n        <td><b>ca</b></td>\n        <td>Number of major vessels (0-4) colored by flourosopy </td>\n        <td>0; 3; ...</td>\n    </tr>\n    <tr>\n        <td><b>thal</b></td>\n        <td>Thalassemia<br><br>3 = normal<br>6 = fixed defect<br>7 = reversable defect</td>\n        <td>1; 3; ...</td>\n    </tr>\n    <tr>\n        <td><b>Target</b></td>\n        <td>Target column<br><br>0 = not have heart disease<br>1 = have heart disease</td>\n        <td>1; 0; ...</td>\n    </tr>\n    </tbody>\n</table>\n<hr>\n<center>\n    <span class=\"thanks-explain\">üìå Like this notebook? You can support me by giving <mark><b>upvote</b></mark> üòÜüëçüîº</span><br>\n    <span class=\"thanks-watermark\">Follow me in other platform: <a href=\"https://linktr.ee/caesarmario_\">linktr.ee/caesarmario_</a></span><br>\n    <span class=\"three-dots2\">...</span><br>\n    <span class=\"thanks-watermark\"><u>Support me!</u></span><br>\n    <span class=\"ko-fi\">\n        <a href='https://ko-fi.com/D1D3JU963' target='_blank'><img src='https://ko-fi.com/img/githubbutton_sm.svg' alt='Support me on Ko-fi Button'/></a><br>\n    </span>\n</center>\n<hr>\n</div>","metadata":{"papermill":{"duration":0.016753,"end_time":"2022-12-02T09:13:49.444787","exception":false,"start_time":"2022-12-02T09:13:49.428034","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <div class=\"header1\">2. | Installing and Importing Libraries üìö</div>\n<div class=\"explain-box\">\n    <b>Installing and Importing libraries</b> that will be used in this notebook.\n</div>","metadata":{"tags":[]}},{"cell_type":"code","source":"# --- Installing Libraries ---\n!pip install ydata-profiling\n!pip install pywaffle\n!pip install highlight-text\n!pip install Pillow","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-04-28T08:02:09.573494Z","iopub.execute_input":"2023-04-28T08:02:09.5739Z","iopub.status.idle":"2023-04-28T08:03:05.701907Z","shell.execute_reply.started":"2023-04-28T08:02:09.573862Z","shell.execute_reply":"2023-04-28T08:03:05.700672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Importing Libraries ---\nimport numpy as np\nimport pandas as pd\nimport ydata_profiling\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nimport warnings\nimport os\nimport yellowbrick\nimport joblib\n\nfrom ydata_profiling import ProfileReport\nfrom pywaffle import Waffle\nfrom statsmodels.graphics.gofplots import qqplot\nfrom PIL import Image\nfrom highlight_text import fig_text\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import RobustScaler, OneHotEncoder \nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom yellowbrick.classifier import PrecisionRecallCurve, ROCAUC, ConfusionMatrix\nfrom yellowbrick.model_selection import LearningCurve, FeatureImportances\nfrom yellowbrick.contrib.wrapper import wrap\nfrom yellowbrick.style import set_palette","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:03:05.703542Z","iopub.execute_input":"2023-04-28T08:03:05.703953Z","iopub.status.idle":"2023-04-28T08:03:08.735939Z","shell.execute_reply.started":"2023-04-28T08:03:05.70391Z","shell.execute_reply":"2023-04-28T08:03:08.734812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div class=\"header1\">3. | Reading Dataset üëì</div>\n<div class=\"explain-box\">\n    After importing libraries, <b>the dataset that will be used will be imported</b>.\n</div>","metadata":{"tags":[]}},{"cell_type":"code","source":"# --- Importing Dataset ---\ndf = pd.read_csv(\"../input/heart-disease/heart.csv\")\n\n# --- Reading Train Dataset ---\nprint(clr.start+'.: Imported Dataset :.'+clr.end)\nprint(clr.color+'*' * 23)\ndf.head().style.background_gradient(cmap='Reds').hide_index()","metadata":{"_kg_hide-input":true,"tags":[],"execution":{"iopub.status.busy":"2023-04-28T08:03:08.737751Z","iopub.execute_input":"2023-04-28T08:03:08.738264Z","iopub.status.idle":"2023-04-28T08:03:08.906866Z","shell.execute_reply.started":"2023-04-28T08:03:08.73821Z","shell.execute_reply":"2023-04-28T08:03:08.905517Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div class=\"header1\">4. | Initial Dataset Exploration üîç</div>\n<div class=\"explain-box\">\n    This section will focused on <b>initial data exploration on the dataset</b> with <u>Pandas Profiling</u> before pre-processing performed. In addition, <b>variables correlation</b> will be examined as well.\n</div>","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[]}},{"cell_type":"code","source":"# --- Dataset Report ---\nProfileReport(df, title='Heart Disease Dataset Report', minimal=True, progress_bar=False, samples=None, correlations=None, interactions=None, explorative=True, dark_mode=True, notebook={'iframe':{'height': '600px'}}, html={'style':{'primary_color': color_line}}, missing_diagrams={'heatmap': False, 'dendrogram': False}).to_notebook_iframe()","metadata":{"_kg_hide-input":true,"tags":[],"execution":{"iopub.status.busy":"2023-04-28T08:03:08.910661Z","iopub.execute_input":"2023-04-28T08:03:08.911851Z","iopub.status.idle":"2023-04-28T08:03:17.571029Z","shell.execute_reply.started":"2023-04-28T08:03:08.911792Z","shell.execute_reply":"2023-04-28T08:03:17.569453Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Correlation Map Variables ---\nsuptitle = dict(x=0.1, y=1.01, fontsize=13, weight='heavy', ha='left', va='bottom', fontname=font_main)\ntitle = dict(x=0.1, y=0.98, fontsize=8, weight='normal', ha='left', va='bottom', fontname=font_alt)\nxy_label = dict(size=6)\nhighlight_textprops = [{'weight':'bold', 'color': colors[0]}, {'weight':'bold', 'color': colors[2]}]\n\n# --- Correlation Map (Heatmap) ---\nmask = np.triu(np.ones_like(df.corr(), dtype=bool))\nfig, ax = plt.subplots(figsize=(7, 6))\nsns.heatmap(df.corr(), mask=mask, annot=True, cmap=color_map, linewidths=0.2, cbar=False, annot_kws={\"size\": 7}, rasterized=True)\nyticks, ylabels = plt.yticks()\nxticks, xlabels = plt.xticks()\nax.set_xticklabels(xlabels, rotation=0, **xy_label)\nax.set_yticklabels(ylabels, **xy_label)\nax.grid(False)\nfig_text(s='Numerical Variables Correlation Map', **suptitle)\nfig_text(s='<Chest pain type, max heart rate, and slope> positively correlate with <target> variables.', highlight_textprops=highlight_textprops, **title)\nplt.tight_layout(rect=[0, 0.04, 1, 1.01])\nplt.gcf().text(0.85, 0.03, 'kaggle.com/caesarmario', style='italic', fontsize=5)\nplt.show();","metadata":{"_kg_hide-input":true,"tags":[],"execution":{"iopub.status.busy":"2023-04-28T08:03:17.573299Z","iopub.execute_input":"2023-04-28T08:03:17.574726Z","iopub.status.idle":"2023-04-28T08:03:20.18994Z","shell.execute_reply.started":"2023-04-28T08:03:17.574635Z","shell.execute_reply":"2023-04-28T08:03:20.188876Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"explain-box\">\n    From <b>dataset report</b> and <b>correlation matrix</b>, it can be <mark><b>concluded</b></mark> that:\n    <blockquote style=\"color: #000000;\">\n        <ul>\n            <li>There are <mark>no missing values</mark> detected in the dataset. In addition, it also can be seen that <b>the number of categorical columns is more than the numerical columns</b>.</li>\n            <li>As can be seen from the profiling report, <mark>the number of male patients is greater than female patients</mark>. In addition, <b>chest pain type 0 (typical angina) is higher than other types</b>. <mark>Most of the patients in the dataset had fasting blood sugar that was less than 120 mg/dl</mark>. <b>The number of resting electrocardiographic types 1 (having ST-T wave abnormality) and 0 (normal) is more than type 2 (definite left ventricular hypertrophy)</b>. Moreover, <mark>patients who don't have exercise-induced angina have a higher number</mark>. <b>The number patients with flat and downsloping slopes is more</b> than upsloping slope.</li>\n            <li>Furthermore, <mark>patients with 0 major vessels are more numerous than those with major vessels</mark>. <b>Patients with fixed defect thalassemia have the highest distribution compared to others</b>. <mark>The total number of patients with heart disease is higher</mark> than those without heart disease.</li>\n            <li><mark>Age, resting blood pressure, cholestoral, and max. heart received columns are lack of variation</mark> since it has <b>low standard deviation</b>.</li>\n            <li><mark>The age column has a normal distribution</mark> based on the histogram and skewness value. However, <b>the resting blood pressure column has a moderately right-skewed distribution and the serum cholestoral and oldpeak columns have a highly right-skewed distribution</b>. On the other hand, <mark>the max. heart received column has a moderate left-skewed distribution</mark>. Since some columns are moderate to highly left or right-skewed, <b>some outliers are detected at the distribution tail</b>.</li>\n            <li><mark>The age, resting blood pressure, max. heart received and oldpeak columns</mark> have a kurtosis value of less than 3, which indicates that the column is <mark>platikurtic</mark>. Meanwhile, <b>the serum cholestoral column</b> has a kurtosis value of more than 3, which indicates that the column is <b>leptokurtic</b>.<br>\n                <blockquote><span style=\"font-size: 11px;\">üìå <mark>Low standard deviation</mark> means data are <mark>clustered around the mean</mark> (lack of variation), and high standard deviation indicates data are more spread out (more variation).</span></blockquote>\n                <blockquote><span style=\"font-size: 11px;\">üìå If skewness is <b>less than -1 or greater than 1</b>, the distribution is <mark>highly skewed</mark>. If skewness is <b>between -1 and -0.5 or between 0.5 and 1</b>, the distribution is <mark>moderately skewed</mark>. If skewness is <b>between -0.5 and 0.5</b>, the distribution is <mark>approximately symmetric</mark>.</span></blockquote>\n                <blockquote><span style=\"font-size: 11px;\">üìå <mark>Kurtosis</mark> values used to show <mark>tailedness of a column</mark>. The value of normal distribution (mesokurtotic) should be equal to 3. If kurtosis value is more than 3, it is called leptokurtic. Meanwhile, if kurtosis value is less than 3, then it is called platikurtic.</span></blockquote>\n            </li>\n            <li><b>The mean age of the patients in the dataset was 54.36 years old</b>, with the most senior patient being 77 years old and the youngest being 29 years old. <b>The average resting blood pressure in the dataset is 131.62</b>, where the highest resting blood pressure is 200, and the minimum is 94 (generally, <b>the ideal blood pressure ranges from 90 to 120</b>).</li>\n            <li><b>The mean serum cholesteral was 246.26</b>, with a maximum of 564 and a minimum of 126. In addition, <b>the patient's average max. heart rate in the dataset was 149.64</b>, with a minimum of 71 and a maximum of 202. <b>The patient's mean oldpeak was 1.03</b>, with a minimum of 0 and a maximum of 6.2.</li>\n            <li>According to the correlation between variables, it can be seen that <b>chest pain type, max. heart rate, and slope have a high positive correlation with the target variable</b>. However, <b>exang, oldpeak, and thalassemia negatively correlate with the target variable</b>.</li>\n        </ul>\n    </blockquote>\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.292259,"end_time":"2022-12-02T09:14:59.621924","exception":false,"start_time":"2022-12-02T09:14:59.329665","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <div class=\"header1\">5. | EDA üìà</div>\n<div class=\"explain-box\">\n    This section will perform some <b>EDA</b> to get more insights about dataset.<br>\n</div>","metadata":{"jp-MarkdownHeadingCollapsed":true,"papermill":{"duration":0.082798,"end_time":"2022-12-02T09:14:46.771098","exception":false,"start_time":"2022-12-02T09:14:46.6883","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <div class=\"header2\">5.1 | Disease Distribution based on Chest Pain Type in Each Gender</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"jp-MarkdownHeadingCollapsed":true,"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- EDA 1 Dataframes ---\ndf_eda1 = df[['sex', 'cp', 'target']]\ndf_eda1 = pd.DataFrame(df_eda1.groupby(['sex', 'target']).cp.value_counts().reset_index(name='total'))\ndf_eda1_mns = df_eda1.query(f'sex == 0 & target == 0')\ndf_eda1_mns.loc[len(df_eda1_mns.index)] = [0, 0, 3, 0]\ndf_eda1_ms = df_eda1.query(f'sex == 0 & target == 1')\ndf_eda1_fns = df_eda1.query(f'sex == 1 & target == 0')\ndf_eda1_fs = df_eda1.query(f'sex == 1 & target == 1')\n\n# --- EDA 1: Variables ---\ny = np.arange(len(df_eda1.cp.unique()))\nx_ticks = list(np.arange(-80, 60, 20))\nx_labels = list(map(str, x_ticks))\nx_labels = list(map(lambda each:each.strip(\"-\"), x_labels))\ny_ticks = list(np.arange(0, 4, 1))\nlabels_pain_type = ['Type 0', 'Type 1', 'Type 2', 'Type 3']\nlabels_legend = ['Not Sick', 'Sick']\nbar_height = 0.35\nbar_style = dict(zorder=3, edgecolor='black', linewidth=0.5, alpha=0.85)\ncnt_label = dict(fontsize=7, horizontalalignment='center', verticalalignment='center')\naxvspan = dict(alpha=0.2, zorder=2)\ntick_params = dict(length=3, width=1, color=color_line)\nxy_label = dict(fontweight='bold', fontsize=8)\nsuptitle = dict(x=0.16, y=0.96, fontsize=13, weight='heavy', ha='left', va='bottom', fontname=font_main)\ntitle = dict(x=0.16, y=0.93, fontsize=8, weight='normal', ha='left', va='bottom', fontname=font_alt)\nhighlight_textprops = [{'weight':'bold', 'color': colors[0]}, {'weight':'bold', 'color': colors[5]}]\n\n# --- Display EDA 1 ---\nfig, ax = plt.subplots(figsize=(9, 5))\nbar_mns = plt.barh(y+bar_height, df_eda1_mns['total'], color=colors[3], height=bar_height, **bar_style) # hatch='//'\nbar_ms = plt.barh(y, df_eda1_ms['total'], color=colors[4], height=bar_height, **bar_style)\nbar_fns = plt.barh(y+bar_height, df_eda1_fns['total']*-1, color=colors[3], height=bar_height, **bar_style)\nbar_fs = plt.barh(y, df_eda1_fs['total']*-1, color=colors[1], height=bar_height, **bar_style)\nax.set_yticks(y + bar_height / 2)\nax.set_yticklabels(labels_pain_type, fontsize=7)\nfor rect in ax.patches:\n    width, height = rect.get_width(), rect.get_height()\n    x, y = rect.get_xy()\n    if width >= 0:\n        if width > 10: ax.text(x+width/2, y+height/2, '{:.0f}'.format(width), **cnt_label)\n        else: ax.text(x+width+1.5, y+height/2, '{:.0f}'.format(width), **cnt_label)\n    elif width < 0:\n        if width*-1 > 10: ax.text(x+width/2, y+height/2, '{:.0f}'.format(width*-1), **cnt_label)\n        else: ax.text(x+width-1.5, y+height/2, '{:.0f}'.format(width*-1), **cnt_label)\nplt.xticks(fontsize=7, ticks=x_ticks, labels=x_labels)\nplt.xlabel('\\nTotal', **xy_label)\nplt.ylabel('Chest Pain Type\\n', **xy_label)\nplt.grid(axis='y', alpha=0, zorder=2)\nplt.grid(axis='x', which='major', alpha=0.3, color=color_grid, linestyle='dotted', zorder=1)\nplt.axvspan(-85, 0, color=colors[1], **axvspan)\nplt.axvspan(40, 0, color=colors[4], **axvspan)\nleg_fsick = mpatches.Patch(color=colors[1], label='Sick Female')\nleg_msick = mpatches.Patch(color=colors[4], label='Sick Male')\nleg_notsick = mpatches.Patch(color=colors[3], label='Not Sick')\nplt.legend(handles=[leg_fsick, leg_msick, leg_notsick], loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3, borderpad=3, frameon=False, fontsize=7, columnspacing=3)\nplt.tick_params(bottom='on', **tick_params)\nax=plt.gca()\nfor spine in ax.spines.values():\n    spine.set_color('None')\nax.spines['bottom'].set_visible(True)\nax.spines['bottom'].set_color(color_line)\nfig_text(s='Disease Distribution based on Chest Pain Type in Each Gender', **suptitle)\nfig_text(s=\"Chest pain types 1, 2, and 3 <have more sick patients> than those <who don't>.\", highlight_textprops=highlight_textprops, **title)\nplt.gcf().text(0.77, -0.09, 'kaggle.com/caesarmario', style='italic', fontsize=6)\nplt.show();","metadata":{"_kg_hide-input":true,"tags":[],"execution":{"iopub.status.busy":"2023-04-28T08:03:20.191224Z","iopub.execute_input":"2023-04-28T08:03:20.192311Z","iopub.status.idle":"2023-04-28T08:03:22.619897Z","shell.execute_reply.started":"2023-04-28T08:03:20.192266Z","shell.execute_reply":"2023-04-28T08:03:22.617811Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"explain-box\">\n    From the butterfly chart above and as previously mentioned, typical angina chest pain and female patients have a greater number in the dataset. When viewed more detail, <mark>atypical angina, non-anginal pain, and asymptomatic chest pain have more sick patients than healthy male and female patients</mark>. In addition, <b>for patients with typical angina chest pain</b>, <mark>the ratio of male and female patients with heart disease is almost the same</mark>. However, <mark>the number of healthy female patients in that chest pain category is higher</mark> than healthy male patients.\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.292259,"end_time":"2022-12-02T09:14:59.621924","exception":false,"start_time":"2022-12-02T09:14:59.329665","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <div class=\"header2\">5.2 | Maximum Heart Rate vs. Age based on Patients Sickness</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"jp-MarkdownHeadingCollapsed":true,"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- EDA 2 Variables ---\nscatter_style = dict(linewidth=0.65, edgecolor=scatter_color_edge, alpha=0.8)\nsub_scatter_style_color = dict(s=5, alpha=0.65, linewidth=0.15, zorder=10, edgecolor=scatter_color_edge)\nsub_scatter_style_grey = dict(s=5, alpha=0.3, linewidth=0.7, zorder=5, color=colors[4])\ngrid_style = dict(alpha=0.3, color=color_grid, linestyle='dotted', zorder=1)\nxy_label = dict(fontweight='bold', fontsize=9)\nsuptitle = dict(x=0.12, y=0.62, fontsize=16, weight='heavy', ha='left', va='bottom', fontname=font_main)\ntitle = dict(x=0.12, y=0.605, fontsize=10, weight='normal', ha='left', va='bottom', fontname=font_alt)\ncolor_pallete = [colors[5], colors[1]]\ntarget_labels = [[0, 1], ['Not Sick', 'Sick']]\nhighlight_textprops = [{'weight':'bold', 'color': colors[5]}, {'weight':'bold', 'color': colors[1]}, {'weight':'bold', 'color': colors[1]}]\nhighlight_mean = [{'fontsize':7, 'color': 'black'}, {'fontsize':8, 'weight':'bold', 'color': colors[5]}]\nsub_axes = [None] * 2\n\n# --- EDA 2 Dataframe & Figure Settings ---\ndf_eda2 = df[['target', 'age', 'thalach']]\nage_mean = df_eda2.age.mean()\nthalach_mean = df_eda2.thalach.mean()\nfig = plt.figure(figsize=(10, 16))\ngs = fig.add_gridspec(2, 2)\nax = fig.add_subplot(gs[:2, :])\nax.set_aspect(1)\n\n# --- EDA 2: Main Scatter Plot ---\nax.axvline(x=thalach_mean, linewidth=0.8, linestyle='--', color=colors[5], alpha=0.5)\nax.axhline(y=age_mean, linewidth=0.8, linestyle='--', color=colors[5], alpha=0.5)\nfor x in range(len(target_labels[0])):\n    df_eda2_temp = df_eda2[df_eda2['target']==target_labels[0][x]]\n    ax.scatter(df_eda2_temp['thalach'], df_eda2_temp['age'], s=65, color=color_pallete[x], **scatter_style)\n    ax.set_xlabel('\\nMaximum Heart Rate', **xy_label)\n    ax.set_ylabel('Age\\n', **xy_label)\n    ax.grid(axis='y', which='major', **grid_style)\n    ax.grid(axis='x', which='major', **grid_style)\n    for spine in ax.spines.values(): spine.set_color('None')\n    for spine in ['bottom', 'left']:\n        ax.spines[spine].set_visible(True)\n        ax.spines[spine].set_color(color_line)\n    plt.tick_params(bottom='on', left='on', **tick_params)\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=8)\nfig_text(x=0.13, y=0.495, ha='left', s=\"<Age Mean:>\\n<{:.2f}>\".format(age_mean), highlight_textprops=highlight_mean)\nfig_text(x=0.59, y=0.426, ha='left', s=\"<Max. Heart Rate Mean:>\\n<{:.2f}>\".format(thalach_mean), highlight_textprops=highlight_mean)\n\n# --- EDA 2: Sub Plots ---\nfor idx, trgt in enumerate(target_labels[0]):\n    gs_thalach = df_eda2[df_eda2['target']!=trgt]['thalach']\n    gs_age = df_eda2[df_eda2['target']!=trgt]['age']\n    cs_thalach = df_eda2[df_eda2['target']==trgt]['thalach']\n    cs_age = df_eda2[df_eda2['target']==trgt]['age']\n\n    sub_axes[idx] = fig.add_subplot(gs[1, idx], aspect=1)\n    sub_axes[idx].scatter(gs_thalach, gs_age, label=trgt, **sub_scatter_style_grey)\n    sub_axes[idx].scatter(cs_thalach, cs_age, color=color_pallete[idx], label=trgt, **sub_scatter_style_color)\n    m, b = np.polyfit(cs_thalach, cs_age, deg=1)\n    sub_axes[idx].plot(cs_thalach, m*cs_thalach+b, linewidth=0.5, color=color_pallete[idx], linestyle='dotted');\n    \n    cnt = (df_eda2['target']==trgt).sum()\n    sub_axes[idx].set_title(f'{target_labels[1][trgt]} Patients - ({cnt})', fontsize=7, style='italic', weight='bold', ha='center')\n    sub_axes[idx].set_xticks([])\n    sub_axes[idx].set_yticks([])\n    for spine in sub_axes[idx].spines.values(): spine.set_color('None')\n\n# --- EDA 2 Titles & WM ---\nfig_text(s='Maximum Heart Rate vs. Age based on Patients Sickness', **suptitle)\nfig_text(s=\"Patients who tend to get <heart disease> are <less than 54 years old> and have <max. heart rate over 149>.\", highlight_textprops=highlight_textprops, **title)\nplt.gcf().text(0.77, 0.23, 'kaggle.com/caesarmario', style='italic', fontsize=7)\nplt.show();","metadata":{"_kg_hide-input":true,"tags":[],"execution":{"iopub.status.busy":"2023-04-28T08:03:22.622396Z","iopub.execute_input":"2023-04-28T08:03:22.623267Z","iopub.status.idle":"2023-04-28T08:03:26.089041Z","shell.execute_reply.started":"2023-04-28T08:03:22.623205Z","shell.execute_reply":"2023-04-28T08:03:26.087415Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"explain-box\">\n    The scatter plot above shows that <b>patients with and without heart disease are aged between 40 to 70 years old</b>. In addition, the spread of <b>max. patient's heart rate in the dataset ranges from 140 to 180</b>. When viewed in more detail, <mark>patients who tend to get heart disease have max. heart rate over 149 and under 54 years of age</mark>. In the scatter plot above, it can also be seen that <mark>age and max. heart rate has a negative correlation</mark>, especially in patients with heart disease. In addition, heart disease patients have more numbers than healthy ones.\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.292259,"end_time":"2022-12-02T09:14:59.621924","exception":false,"start_time":"2022-12-02T09:14:59.329665","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <div class=\"header2\">5.3 | Fasting Blood Sugar Distribution by Resting Electrocardiographic Results</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"jp-MarkdownHeadingCollapsed":true,"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- EDA 3 Dataframes ---\ndf_eda3 = df[['fbs', 'restecg']]\ndf_eda3 = pd.DataFrame(df_eda3.groupby(['fbs', 'restecg']).size().reset_index(name='total'))\ndf_eda3.loc[len(df_eda3.index)] = [1, 2, 0]\ndf_eda3_0 = df_eda3.query(f'restecg == 0').drop('restecg', axis=1)\ndf_eda3_1 = df_eda3.query(f'restecg == 1').drop('restecg', axis=1)\ndf_eda3_2 = df_eda3.query(f'restecg == 2').drop('restecg', axis=1)\n\n# --- EDA 3 Variables ---\ntotal_list = [df_eda3_0['total'], df_eda3_1['total'], df_eda3_2['total']]\nsuptitle = dict(x=0.5, y=0.94, fontsize=14, weight='heavy', ha='center', va='center', fontname=font_main)\nexp_text = dict(x=0.5, y=0.17, fontsize=6, weight='normal', ha='center', va='center', textalign='center', fontname=font_alt)\nhighlight_explanation = [{'weight':'bold', 'color': colors[5]}, {'weight':'bold', 'color': colors[5]}, {'weight':'bold', 'color': colors[1]}]\nl_120mg = mpatches.Patch(color=colors[5], label='< 120 mg/dl')\nm_120mg = mpatches.Patch(color=colors[1], label='> 120 mg/dl')\n\n# --- EDA 3 Functions ---\ndef display_eda3(subplot_num, restecg_type, total, colors, start_angle):\n    centre = plt.Circle((0, 0), 0.85, fc='white', edgecolor='black', linewidth=0.5)\n    total_patients = total.sum()\n    \n    plt.subplot(1, 3, subplot_num)\n    plt.tight_layout(rect=[0, 0, 1, 1.01])\n    plt.pie(total, colors=colors, autopct='%.2f%%', pctdistance=0.65, startangle=start_angle, wedgeprops=dict(alpha=0.85, edgecolor='black', linewidth=0.5), textprops={'fontsize': 7, 'fontname': font_alt})\n    plt.text(0, 0.08, f\"Type {restecg_type}\", weight='bold', ha='center', fontsize=10, fontname=font_main)\n    plt.text(0, -0.08, f\"{total_patients} patients\", ha='center', fontsize=8, fontname=font_alt)\n    fig=plt.gcf()\n    fig.gca().add_artist(centre)\n\n# --- Display EDA 3 ---\nplt.figure(figsize=(9, 4))\nfor idx, total in enumerate(total_list):\n    display_eda3(idx+1, idx, total, [colors[5], colors[1]], sample_num[idx])\n    if idx == 1: plt.legend(handles=[l_120mg, m_120mg], loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2, borderpad=3, frameon=False, fontsize=7, columnspacing=3)\nfig_text(s=\"Fasting Blood Sugar Distribution by Resting Electrocardiographic Results\", **suptitle)\nfig_text(s=\"<Resting electrocardiograph type 0 and 1 have higher distribution> compared to type 2.\\n<Only type 0 and 1 have patients with fasting blood sugar over 120 mg/dl>, while <type 2 does not>\", highlight_textprops=highlight_explanation, **exp_text)\nplt.gcf().text(0.83, 0.07, 'kaggle.com/caesarmario', style='italic', fontsize=7)\nplt.show();","metadata":{"_kg_hide-input":true,"tags":[],"execution":{"iopub.status.busy":"2023-04-28T08:03:26.090808Z","iopub.execute_input":"2023-04-28T08:03:26.09124Z","iopub.status.idle":"2023-04-28T08:03:28.17329Z","shell.execute_reply.started":"2023-04-28T08:03:26.091198Z","shell.execute_reply":"2023-04-28T08:03:28.171625Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"explain-box\">\n    The donut chart above shows <b>resting electrocardiograph types 0 and 1 have almost the same number of patients</b>. However, inversely proportional to resting electrocardiograph type 2, where the number of patients is only four. In addition, <mark>resting electrocardiograph types 0 and 1 have patients with fasting blood sugar over 120 mg/dl</mark>. Although, resting electrocardiograph type 2 had no patients with fasting blood sugar over 120 mg/dl.\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.292259,"end_time":"2022-12-02T09:14:59.621924","exception":false,"start_time":"2022-12-02T09:14:59.329665","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <div class=\"header2\">5.4 | Number of Major Vessles Distribution based on Exercise Induced Angina</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"jp-MarkdownHeadingCollapsed":true,"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- EDA 4 Dataframes ---\ndf_eda4 = df[['exang', 'ca']]\ndf_eda4 = pd.DataFrame(df_eda4.groupby(['exang', 'ca']).size().reset_index(name='total'))\ndf_eda4_0 = df_eda4.query(f'exang == 0').drop(['exang', 'ca'], axis=1).reset_index(drop=True)\ndf_eda4_1 = df_eda4.query(f'exang == 1').drop(['exang', 'ca'], axis=1).reset_index(drop=True)\n\n# --- EDA 4 Variables ---\nsuptitle = dict(x=0.3, y=1.07, fontsize=48, weight='heavy', ha='center', va='center', fontname=font_main)\ntitle = dict(x=0.3, y=1.01, fontsize=30, weight='normal', ha='center', va='bottom', fontname=font_alt)\ntitle_pywaffle = dict(loc='left', fontsize=30, weight='bold', fontname=font_main)\nlegend_pywaffle = dict(loc='upper center', fontsize=22, ncol=5, borderpad=3, frameon=False, columnspacing=3)\n\n# --- Display EDA 4 ---\nfig = plt.figure(FigureClass=Waffle,\n    plots={211: {'values': df_eda4_0['total'], \n                 'labels': [f\"{key} Major Vessels - ({value})\" for key, value in df_eda4_0['total'].items()], \n                 'legend': {'bbox_to_anchor': (0.5, 0.05), **legend_pywaffle},\n                 'title': {'label': \"Don't Have Exercise Induced Angina\\n\", **title_pywaffle}}\n           , 212: {'values': df_eda4_1['total'], \n                   'labels': [f\"{key} Major Vessels - ({value})\" for key, value in df_eda4_1['total'].items()], \n                   'legend': {'bbox_to_anchor': (1, 0.05), **legend_pywaffle},\n                   'title': {'label': \"Have Exercise Induced Angina\\n\", **title_pywaffle}}\n          }, figsize=(50, 20), rows=7, colors=color_pywaffle, rounding_rule='ceil')\nfig.suptitle('\\nNumber of Major Vessles Distribution based on Exercise Induced Angina', **suptitle)\nplt.gcf().text(s='The major vessel distribution proportion in patients with and without exercise-induced angina is almost the same.', **title)\nplt.gcf().text(0.48, 0.01, 'kaggle.com/caesarmario', style='italic', fontsize=20)\nfig.tight_layout()\nplt.show();","metadata":{"_kg_hide-input":true,"tags":[],"execution":{"iopub.status.busy":"2023-04-28T08:03:28.175262Z","iopub.execute_input":"2023-04-28T08:03:28.175824Z","iopub.status.idle":"2023-04-28T08:03:47.077732Z","shell.execute_reply.started":"2023-04-28T08:03:28.175784Z","shell.execute_reply":"2023-04-28T08:03:47.075438Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"explain-box\">\n    The waffle charts above show that <mark>the proportion between patients who do and do not do exercise-induced angina is almost the same</mark>. This can be seen by <b>comparing the total number of patients between major vessels in each exercise</b>.\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.292259,"end_time":"2022-12-02T09:14:59.621924","exception":false,"start_time":"2022-12-02T09:14:59.329665","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <div class=\"header2\">5.5 | Resting Blood Pressure Distribution based on Slope</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"jp-MarkdownHeadingCollapsed":true,"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- EDA 5 Dataframes ---\ndf_eda5 = df[['slope', 'trestbps']]\ndf_eda5['slope'] = df_eda5['slope'].astype(str)\n\n# --- EDA 5 Variables ---\ntick_params=dict(length=3, width=1, color=color_line)\nxy_label=dict(fontweight='bold', fontsize=7)\nslope_list = sorted(df_eda5['slope'].unique())\ncolor_pallete = [colors[5], colors[4], colors[0]]\nsub_axes=[None] * 3\nsuptitle = dict(x=0.125, y=0.925, fontsize=14, weight='heavy', ha='left', va='bottom', fontname=font_main)\ntitle = dict(x=0.125, y=0.9, fontsize=8, weight='normal', ha='left', va='bottom', fontname=font_alt)\nqq_plot = dict(fit=True, line='45', markeredgecolor=scatter_color_edge)\nhighlight_textprops = [{'weight':'bold', 'color': colors[5]}, {'weight':'bold', 'color': colors[1]}]\n\n# --- EDA 5 Settings ---\nfig = plt.figure(figsize=(10, 7))\ngs = fig.add_gridspec(6, 3)\nax = fig.add_subplot(gs[:3, :])\n\n# --- EDA 5: Main KDE Plot ---\nsns.kdeplot(x='trestbps', hue='slope', data=df_eda5, palette=color_pallete, hue_order=slope_list, bw_adjust=0.4, fill=True, ax=ax)\nplt.legend([], [], frameon=False)\nplt.grid(axis='x', which='major', alpha=0.75, color=color_line, linestyle='dotted', zorder=1)\nplt.grid(axis='y', alpha=0, zorder=2)\nplt.xticks(fontsize=6)\nplt.yticks(fontsize=6)\nplt.xlabel('\\nResting Blood Pressure (in mm Hg)', **xy_label)\nplt.ylabel('Density\\n', **xy_label)\nplt.tick_params(left='on', bottom='on', **tick_params)\nfor spine in ax.spines.values(): spine.set_color('None')\nfor spine in ['bottom', 'left']:\n    ax.spines[spine].set_visible(True)\n    ax.spines[spine].set_color(color_line)\nfig_text(s='Resting Blood Pressure Distribution based on Slope', **suptitle)\nfig_text(s='Each <slope type distribution> is <moderately right-skewed>.', highlight_textprops=highlight_textprops, **title)\nplt.gcf().text(0.79, 0.16, 'kaggle.com/caesarmario', style='italic', fontsize=7)\n\n# --- EDA 5: Sub Q-Q Plot ---\nfor idx, slp in enumerate(slope_list):\n    df_eda5_slope = df_eda5[df_eda5['slope']==slp]\n    sub_axes[idx] = fig.add_subplot(gs[4, idx])\n    qqplot(df_eda5['trestbps'], ax=sub_axes[idx], markerfacecolor=color_line, alpha=0.4, **qq_plot)\n    qqplot(df_eda5_slope['trestbps'], ax=sub_axes[idx], markerfacecolor=color_pallete[idx], alpha=0.5, **qq_plot)\n    for line in [1, 3]:\n        sub_axes[idx].get_lines()[line].set_color(colors[5])\n        sub_axes[idx].get_lines()[line].set_linewidth(0.8)\n        sub_axes[idx].get_lines()[line].set_linestyle('--')\n    sub_axes[idx].set_xticks([])\n    sub_axes[idx].set_yticks([])\n    sub_axes[idx].set_xlabel('')\n    sub_axes[idx].set_ylabel('')\n    sub_axes[idx].legend([], [], frameon=False)\n    sub_axes[idx].set_title(f'Q-Q Plot - Slope {slp}', fontsize=8, style='italic', weight='bold', ha='center')\n    for spines in sub_axes[idx].spines.values(): spines.set_color('None')\nplt.show();","metadata":{"_kg_hide-input":true,"tags":[],"execution":{"iopub.status.busy":"2023-04-28T08:03:47.084153Z","iopub.execute_input":"2023-04-28T08:03:47.084708Z","iopub.status.idle":"2023-04-28T08:03:49.696416Z","shell.execute_reply.started":"2023-04-28T08:03:47.084663Z","shell.execute_reply":"2023-04-28T08:03:49.694484Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"explain-box\">\n    The distribution plot and Q-Q plots above show that each slope type's distribution is <mark>moderately right-skewed</mark>. This is due to <b>outliers</b> (distribution tail) on the right side of the plot. In addition, <b>the skewness value and gap at the upper of Q-Q plots with a 45-degree line</b> also show that the distribution in this column is not normal.\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.292259,"end_time":"2022-12-02T09:14:59.621924","exception":false,"start_time":"2022-12-02T09:14:59.329665","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <div class=\"header1\">6. | Data Preprocessing ‚öôÔ∏è</div>\n<div class=\"explain-box\">\n    This section will <b>prepare the dataset</b> before building the machine learning models.\n</div>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## <div class=\"header2\">6.1 | Features Separating and Splitting ü™ì</div>\n<div class=\"explain-box\">\n     In this section, <mark>the 'target' (dependent) column will be seperated from independent columns</mark>. Also, the dataset will be splitted into <mark>80:20 ratio</mark> (80% training and 20% testing).\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- Seperating Dependent Features ---\nx = df.drop(['target'], axis=1)\ny = df['target']\n\n# --- Splitting Dataset ---\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:03:49.698177Z","iopub.execute_input":"2023-04-28T08:03:49.698589Z","iopub.status.idle":"2023-04-28T08:03:49.708975Z","shell.execute_reply.started":"2023-04-28T08:03:49.698553Z","shell.execute_reply":"2023-04-28T08:03:49.707592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div class=\"header2\">6.2 | Processing Pipeline ü™†</div>\n<div class=\"explain-box\">\n    This section will <mark>create a preprocessing pipeline</mark> for numerical and categorical columns and <mark>apply them to the <code>x_train</code> and <code>x_test</code> data</mark>. Not all columns will go through preprocessing. For <mark>all numerical columns</mark>, scaling will be carried out using a <mark>robust scaler</mark> since the dataset used is a <b>small dataset</b> where the presence of outliers dramatically affects the performance of a model. While for <mark>categorical columns with more than two categories, one-hot encoding will be carried out</mark>.</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- Numerical Pipeline ---\nnum_column = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\nnum_pipeline = Pipeline([\n    ('scaling', RobustScaler())\n])\n\n# --- Categorical Pipeline ---\ncat_column = ['cp', 'slope', 'thal']\ncat_pipeline = Pipeline([\n    ('onehot', OneHotEncoder(drop='first', sparse=False))\n])\n\n# --- Combine Both Pipelines into Transformer ---\npreprocessor = ColumnTransformer([\n    ('categorical', cat_pipeline, cat_column)\n    , ('numerical', num_pipeline, num_column)]\n    , remainder='passthrough')\n\n# --- Apply Transformer to Pipeline ---\nprocess_pipeline = Pipeline([\n    ('preprocessor', preprocessor)\n])\n\n# --- Apply to Dataframe --- \nx_train_process = process_pipeline.fit_transform(x_train)\nx_test_process = process_pipeline.fit_transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:03:49.710389Z","iopub.execute_input":"2023-04-28T08:03:49.710781Z","iopub.status.idle":"2023-04-28T08:03:49.752819Z","shell.execute_reply.started":"2023-04-28T08:03:49.710741Z","shell.execute_reply":"2023-04-28T08:03:49.751436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div class=\"header1\">7. | Model Implementation üõ†Ô∏è</div>\n<div class=\"explain-box\">\n    This section will <b>implement various machine learning models</b> as mentioned in Introduction section. In addition, explanation for each models also will be discussed.\n</div>","metadata":{"tags":[]}},{"cell_type":"code","source":"# --- Functions: Model Fitting and Performance Evaluation ---\ndef fit_ml_models(algo, algo_param, algo_name):\n    \n    # --- Algorithm Pipeline ---\n    algo = Pipeline([('algo', algo)])\n    \n    # --- Apply Grid Search ---\n    model = GridSearchCV(algo, param_grid=algo_param, cv=10, n_jobs=-1, verbose=1)\n    \n    # --- Fitting Model ---\n    print(clr.start+f\".:. Fitting {algo_name} .:.\"+clr.end)\n    fit_model = model.fit(x_train_process, y_train)\n    \n    # --- Model Best Parameters ---\n    best_params = model.best_params_\n    print(\"\\n>> Best Parameters: \"+clr.start+f\"{best_params}\"+clr.end)\n    \n    # --- Best & Final Estimators ---\n    best_model = model.best_estimator_\n    best_estimator = model.best_estimator_._final_estimator\n    best_score = round(model.best_score_, 4)\n    print(\">> Best Score: \"+clr.start+\"{:.3f}\".format(best_score)+clr.end)\n    \n    # --- Create Prediction for Train & Test ---\n    y_pred_train = model.predict(x_train_process)\n    y_pred_test = model.predict(x_test_process)\n    \n    # --- Train & Test Accuracy Score ---\n    acc_score_train = round(accuracy_score(y_pred_train, y_train)*100, 3)\n    acc_score_test = round(accuracy_score(y_pred_test, y_test)*100, 3)\n    print(\"\\n\"+clr.start+f\".:. Train and Test Accuracy Score for {algo_name} .:.\"+clr.end)\n    print(\"\\t>> Train Accuracy: \"+clr.start+\"{:.2f}%\".format(acc_score_train)+clr.end)\n    print(\"\\t>> Test Accuracy: \"+clr.start+\"{:.2f}%\".format(acc_score_test)+clr.end)\n    \n    # --- Classification Report ---\n    print(\"\\n\"+clr.start+f\".:. Classification Report for {algo_name} .:.\"+clr.end)\n    print(classification_report(y_test, y_pred_test))\n    \n    # --- Figures Settings ---\n    xy_label = dict(fontweight='bold', fontsize=12)\n    grid_style = dict(color=color_grid, linestyle='dotted', zorder=1)\n    title_style = dict(fontsize=14, fontweight='bold')\n    tick_params = dict(length=3, width=1, color=color_line)\n    bar_style = dict(zorder=3, edgecolor='black', linewidth=0.5, alpha=0.85)\n    set_palette(color_yb)\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 14))\n    \n    # --- Confusion Matrix ---\n    conf_matrix = ConfusionMatrix(best_estimator, ax=ax1, cmap='Reds')\n    conf_matrix.fit(x_train_process, y_train)\n    conf_matrix.score(x_test_process, y_test)\n    conf_matrix.finalize()\n    conf_matrix.ax.set_title('Confusion Matrix\\n', **title_style)\n    conf_matrix.ax.tick_params(axis='both', labelsize=10, bottom='on', left='on', **tick_params)\n    for spine in conf_matrix.ax.spines.values(): spine.set_color(color_line)\n    conf_matrix.ax.set_xlabel('\\nPredicted Class', **xy_label)\n    conf_matrix.ax.set_ylabel('True Class\\n', **xy_label)\n    conf_matrix.ax.xaxis.set_ticklabels(['False', 'True'], rotation=0)\n    conf_matrix.ax.yaxis.set_ticklabels(['True', 'False'])\n    \n    # --- ROC AUC ---\n    logrocauc = ROCAUC(best_estimator, classes=['False', 'True'], ax=ax2, colors=color_yb)\n    logrocauc.fit(x_train_process, y_train)\n    logrocauc.score(x_test_process, y_test)\n    logrocauc.finalize()\n    logrocauc.ax.set_title('ROC AUC Curve\\n', **title_style)\n    logrocauc.ax.tick_params(axis='both', labelsize=10, bottom='on', left='on', **tick_params)\n    logrocauc.ax.grid(axis='both', alpha=0.4, **grid_style)\n    for spine in logrocauc.ax.spines.values(): spine.set_color('None')\n    for spine in ['bottom', 'left']:\n        logrocauc.ax.spines[spine].set_visible(True)\n        logrocauc.ax.spines[spine].set_color(color_line)\n    logrocauc.ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2, borderpad=2, frameon=False, fontsize=10)\n    logrocauc.ax.set_xlabel('\\nFalse Positive Rate', **xy_label)\n    logrocauc.ax.set_ylabel('True Positive Rate\\n', **xy_label)\n    \n    # --- Learning Curve ---\n    lcurve = LearningCurve(best_estimator, scoring='f1_weighted', ax=ax3, colors=color_yb)\n    lcurve.fit(x_train_process, y_train)\n    lcurve.finalize()\n    lcurve.ax.set_title('Learning Curve\\n', **title_style)\n    lcurve.ax.tick_params(axis='both', labelsize=10, bottom='on', left='on', **tick_params)\n    lcurve.ax.grid(axis='both', alpha=0.4, **grid_style)\n    for spine in lcurve.ax.spines.values(): spine.set_color('None')\n    for spine in ['bottom', 'left']:\n        lcurve.ax.spines[spine].set_visible(True)\n        lcurve.ax.spines[spine].set_color(color_line)\n    lcurve.ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2, borderpad=2, frameon=False, fontsize=10)\n    lcurve.ax.set_xlabel('\\nTraining Instances', **xy_label)\n    lcurve.ax.set_ylabel('Scores\\n', **xy_label)\n    \n    # --- Feature Importance or Precision Recall Curve ---\n    try:\n        feat_importance = FeatureImportances(best_estimator, labels=columns_list_onehot, ax=ax4, topn=5, colors=color_yb_importance)\n        feat_importance.fit(x_train_process, y_train)\n        feat_importance.finalize()\n        feat_importance.ax.set_title('Feature Importances (Top 5 Features)\\n', **title_style)\n        feat_importance.ax.tick_params(axis='both', labelsize=10, bottom='on', left='on', **tick_params)\n        feat_importance.ax.grid(axis='x', alpha=0.4, **grid_style)\n        feat_importance.ax.grid(axis='y', alpha=0, **grid_style)\n        for spine in feat_importance.ax.spines.values(): spine.set_color('None')\n        for spine in ['bottom']:\n            feat_importance.ax.spines[spine].set_visible(True)\n            feat_importance.ax.spines[spine].set_color(color_line)\n        feat_importance.ax.set_xlabel('\\nRelative Importance', **xy_label)\n        feat_importance.ax.set_ylabel('Features\\n', **xy_label)\n    except:\n        prec_curve = PrecisionRecallCurve(best_estimator, ax=ax4, ap_score=True, iso_f1_curves=True)\n        prec_curve.fit(x_train_process, y_train)\n        prec_curve.score(x_test_process, y_test)\n        prec_curve.finalize()\n        prec_curve.ax.set_title('Precision-Recall Curve\\n', **title_style)\n        prec_curve.ax.tick_params(axis='both', labelsize=10, bottom='on', left='on', **tick_params)\n        for spine in prec_curve.ax.spines.values(): spine.set_color('None')\n        for spine in ['bottom', 'left']:\n            prec_curve.ax.spines[spine].set_visible(True)\n            prec_curve.ax.spines[spine].set_color(color_line)\n        prec_curve.ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2, borderpad=2, frameon=False, fontsize=10)\n        prec_curve.ax.set_xlabel('\\nRecall', **xy_label)\n        prec_curve.ax.set_ylabel('Precision\\n', **xy_label)\n        \n    plt.suptitle(f'\\n{algo_name} Performance Evaluation Report\\n', fontsize=18, fontweight='bold')\n    plt.gcf().text(0.88, 0.02, 'kaggle.com/caesarmario', style='italic', fontsize=10)\n    plt.tight_layout();\n    \n    return acc_score_train, acc_score_test, best_score","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-28T08:03:49.759647Z","iopub.execute_input":"2023-04-28T08:03:49.7601Z","iopub.status.idle":"2023-04-28T08:03:49.797814Z","shell.execute_reply.started":"2023-04-28T08:03:49.760059Z","shell.execute_reply":"2023-04-28T08:03:49.796489Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div class=\"header2\">7.1 | Logistic Regression</div>\n<div class=\"explain-box\">\n    <blockquote style=\"color: #000000;\">\n        <mark><b>Logistic regression</b></mark> is a statistical method that is used for building machine learning models where <b>the dependent variable is dichotomous: i.e. binary</b>. Logistic regression is used to describe data and <b>the relationship between one dependent variable and one or more independent variables</b>. The independent variables can be nominal, ordinal, or of interval type.<br><br>\n    The name \"logistic regression\" is derived from the concept of the logistic function that it uses. <b>The logistic function is also known as the sigmoid function</b>. The value of this logistic function lies between zero and one.<br><br>\n        <center>\n            <img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/years-2.JPG\" alt=\"Logistic Regression\" width=\"40%\"><br>\n            <i>üñº Logistic Function by Simplilearn</i>\n        </center>\n    </blockquote>\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- Logistic Regression Parameters ---\nparameter_lr = {\"algo__solver\": [\"lbfgs\", \"saga\", \"newton-cg\"]\n                , \"algo__C\": [0.1, 0.2, 0.5, 0.8]}\n\n# --- Logistic Regression Algorithm ---\nalgo_lr = LogisticRegression(penalty=\"l2\", random_state=42, n_jobs=-1)\n\n# --- Applying Logistic Regression ---\nacc_score_train_lr, acc_score_test_lr, best_score_lr = fit_ml_models(algo_lr, parameter_lr, \"Logistic Regression\")","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:03:49.800513Z","iopub.execute_input":"2023-04-28T08:03:49.801262Z","iopub.status.idle":"2023-04-28T08:04:04.961948Z","shell.execute_reply.started":"2023-04-28T08:03:49.801202Z","shell.execute_reply":"2023-04-28T08:04:04.960302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div class=\"header2\">7.2 | K-Nearest Neighbour (KNN)</div>\n<div class=\"explain-box\">\n    <blockquote style=\"color: #000000;\">\n        <mark><b>The k-nearest neighbors (KNN)</b></mark> algorithm is a data classification method <b>for estimating the likelihood that a data point will become a member of one group or another</b> based on what group the data points nearest to it belong to. The k-nearest neighbor algorithm is a type of supervised machine learning algorithm used <b>to solve classification and regression problems</b>.<br><br>\n    It's called a <b>lazy learning algorithm or lazy learner</b> because it doesn't perform any training when you supply the training data. Instead, it just stores the data during the training time and doesn't perform any calculations. It doesn't build a model until a query is performed on the dataset. This makes KNN ideal for data mining.<br><br>\n        <center>\n            <img src=\"https://1.bp.blogspot.com/-D6REhf2XBwQ/XZcWn0cwSEI/AAAAAAAAAvs/LUCN8jxvzcMjkkDK4FAXSuR7MBDW8SBJgCLcBGAsYHQ/s1600/KNN_final_a1mrv9.jpg\" alt=\"KNN\" width=\"35%\"><br>\n            <i>üñº KNN by Kita Informatika</i>\n        </center>\n    </blockquote>\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- KNN Parameters ---\nparameter_knn = {\"algo__n_neighbors\": [2, 5, 10, 17]\n                , \"algo__leaf_size\": [1, 10, 11, 30]}\n\n# --- KNN Algorithm ---\nalgo_knn = KNeighborsClassifier(n_jobs=-1)\n\n# --- Applying KNN ---\nacc_score_train_knn, acc_score_test_knn, best_score_knn = fit_ml_models(algo_knn, parameter_knn, \"K-Nearest Neighbour (KNN)\")","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:04:04.964413Z","iopub.execute_input":"2023-04-28T08:04:04.964808Z","iopub.status.idle":"2023-04-28T08:04:21.355893Z","shell.execute_reply.started":"2023-04-28T08:04:04.96477Z","shell.execute_reply":"2023-04-28T08:04:21.352776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div class=\"header2\">7.3 | Support Vector Machine (SVM)</div>\n<div class=\"explain-box\">\n    <blockquote style=\"color: #000000;\">\n        <mark><b>Support Vector Machine (SVM)</b></mark> is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. The goal of the SVM algorithm is <b>to create the best line or decision boundary that can segregate n-dimensional space into classes</b> so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane.<br><br>\n        SVM chooses the <b>extreme points/vectors</b> that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as Support Vector Machine.<br>\n        <center>\n            <img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm.png\" alt=\"SVM\" width=\"40%\"><br>\n            <i>üñº SVM by JavaTPoint</i>\n        </center>\n    </blockquote>\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- SVM Parameters ---\nparameter_svc = [\n    {'algo__kernel': ['rbf'], 'algo__gamma': np.arange(0.1, 1, 0.1), 'algo__C': np.arange(0.1, 1, 0.1)}\n    , {'algo__kernel': ['linear'], 'algo__C': np.arange(0.1, 1, 0.1)}\n    , {'algo__kernel': ['poly'], 'algo__degree' : np.arange(1, 10, 1), 'algo__C': np.arange(0.1, 1, 0.1)}\n]\n\n# --- SVM Algorithm ---\nalgo_svc = SVC(random_state=1, probability=True)\n\n# --- Applying SVM ---\nacc_score_train_svc, acc_score_test_svc, best_score_svc = fit_ml_models(algo_svc, parameter_svc, \"Support Vector Machine (SVM)\")","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:04:21.357592Z","iopub.execute_input":"2023-04-28T08:04:21.358Z","iopub.status.idle":"2023-04-28T08:04:43.137083Z","shell.execute_reply.started":"2023-04-28T08:04:21.357961Z","shell.execute_reply":"2023-04-28T08:04:43.135597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div class=\"header2\">7.4 | Gaussian Naive Bayes</div>\n<div class=\"explain-box\">\n    <blockquote style=\"color: #000000;\">\n        <mark><b>Naive Bayes Classifiers</b></mark> are based on the Bayes Theorem, which <b>one assumption taken is the strong independence assumptions between the features</b>. These classifiers assume that the value of a particular feature is independent of the value of any other feature. In a supervised learning situation, Naive Bayes Classifiers are trained very efficiently. Naive Bayes classifiers <b>need a small training data to estimate the parameters needed for classification</b>. Naive Bayes Classifiers have simple design and implementation and they can applied to many real life situations.<br><br>\n        <mark><b>Gaussian Naive Bayes</b></mark> is a <b>variant of Naive Bayes that follows Gaussian normal distribution and supports continuous data</b>. When working with continuous data, an assumption often taken is that the continuous values associated with each class are distributed according to a normal (or Gaussian) distribution.<br>\n        <center>\n            <img src=\"https://iq.opengenus.org/content/images/2020/02/Illustration-of-how-a-Gaussian-Naive-Bayes-GNB-classifier-works-For-each-data-point.png\" alt=\"GNB\" width=\"35%\"><br>\n            <i>üñº Gaussian Naive Bayes by OpenGenus</i>\n        </center>\n    </blockquote>\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- Gaussian NB Parameters ---\nparameter_gnb = {\"algo__var_smoothing\": [1e-2, 1e-3, 1e-4, 1e-6]}\n\n# --- Gaussian NB Algorithm ---\nalgo_gnb = GaussianNB()\n\n# --- Applying Gaussian NB ---\nacc_score_train_gnb, acc_score_test_gnb, best_score_gnb = fit_ml_models(algo_gnb, parameter_gnb, \"Gaussian Naive Bayes\")","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:04:43.139183Z","iopub.execute_input":"2023-04-28T08:04:43.140148Z","iopub.status.idle":"2023-04-28T08:04:53.29404Z","shell.execute_reply.started":"2023-04-28T08:04:43.14007Z","shell.execute_reply":"2023-04-28T08:04:53.291743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div class=\"header2\">7.5 | Decision Tree</div>\n<div class=\"explain-box\">\n    <blockquote style=\"color: #000000;\">\n        <mark><b>Decision Tree</b></mark> is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where <b>internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome</b>.<br><br>\n    In a Decision tree, there are <b>two nodes</b>, which are the <mark><b>Decision Node and Leaf Node</b></mark>. Decision nodes are used to make any decision and have multiple branches, whereas Leaf nodes are the output of those decisions and do not contain any further branches.<br>\n        <center>\n            <img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/decision-tree-classification-algorithm.png\" alt=\"DT\" width=\"35%\"><br>\n            <i>üñº Decision Tree by Javatpoint</i>\n        </center>\n    </blockquote>\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- Decision Tree Parameters ---\nparameter_dt = {\"algo__max_depth\": [1, 2, 3]}\n\n# --- Decision Tree Algorithm ---\nalgo_dt = DecisionTreeClassifier(random_state=42)\n\n# --- Applying Decision Tree ---\nacc_score_train_dt, acc_score_test_dt, best_score_dt = fit_ml_models(algo_dt, parameter_dt, \"Decision Tree\")","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:04:53.29605Z","iopub.execute_input":"2023-04-28T08:04:53.296446Z","iopub.status.idle":"2023-04-28T08:05:02.738131Z","shell.execute_reply.started":"2023-04-28T08:04:53.296408Z","shell.execute_reply":"2023-04-28T08:05:02.736201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div class=\"header2\">7.6 | Random Forest</div>\n<div class=\"explain-box\">\n    <blockquote style=\"color: #000000;\">\n        <mark><b>Random Forest</b></mark> is a tree-based machine learning algorithm that <b>leverages the power of multiple decision trees for making decisions</b>. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model‚Äôs prediction. <b>A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models</b>.<br>\n        <center>\n            <img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/rfc_vs_dt1.png\" alt=\"RF\" width=\"35%\"><br>\n            <i>üñº Random Forest by Abhishek Sharma</i>\n        </center>\n    </blockquote>\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- Random Forest Parameters ---\nparameter_rf = {\"algo__max_depth\": np.arange(1, 6, 1)}\n\n# --- Random Forest Algorithm ---\nalgo_rf = RandomForestClassifier(random_state=99, n_jobs=-1)\n\n# --- Applying Random Forest ---\nacc_score_train_rf, acc_score_test_rf, best_score_rf = fit_ml_models(algo_rf, parameter_rf, \"Random Forest\")","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:05:02.739832Z","iopub.execute_input":"2023-04-28T08:05:02.740391Z","iopub.status.idle":"2023-04-28T08:05:21.778181Z","shell.execute_reply.started":"2023-04-28T08:05:02.740332Z","shell.execute_reply":"2023-04-28T08:05:21.77685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div class=\"header2\">7.7 | Extra Tree Classifier</div>\n<div class=\"explain-box\">\n    <blockquote style=\"color: #000000;\">\n        <mark><b>Extra Trees Classifier</b></mark> is a type of ensemble learning technique which <b>aggregates the results of multiple de-correlated decision trees collected in a \"forest\" to output it‚Äôs classification result</b>. In concept, it is very similar to a Random Forest Classifier and only differs from it in the manner of construction of the decision trees in the forest.<br><br>\n        Each Decision Tree in the Extra Trees Forest is <b>constructed from the original training sample</b>. Then, at each test node, each tree is provided with a <b>random sample of k features</b> from the feature-set from which each decision tree must select the best feature to split the data based on some mathematical criteria (typically the Gini Index). This random sample of features leads to the creation of multiple de-correlated decision trees.\n    </blockquote>\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- Extra Tree Parameters ---\nparameter_et = {\"algo__max_depth\": [2, 3]\n    , \"algo__max_leaf_nodes\": [3, 5, 7]}\n\n# --- Extra Tree Algorithm ---\nalgo_et = ExtraTreesClassifier(random_state=42, n_jobs=-1)\n\n# --- Applying Extra Tree ---\nacc_score_train_et, acc_score_test_et, best_score_et = fit_ml_models(algo_et, parameter_et, \"Extra Tree Classifier\")","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:05:21.780104Z","iopub.execute_input":"2023-04-28T08:05:21.780904Z","iopub.status.idle":"2023-04-28T08:05:40.627499Z","shell.execute_reply.started":"2023-04-28T08:05:21.780852Z","shell.execute_reply":"2023-04-28T08:05:40.625573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div class=\"header2\">7.8 | Gradient Boosting</div>\n<div class=\"explain-box\">\n    <blockquote style=\"color: #000000;\">\n        <mark><b>Boosting</b></mark> is a method of <b>converting weak learners into strong learners</b>. In boosting, <b>each new tree is a fit on a modified version</b> of the original data set. It strongly relies on the prediction that the next model will reduce prediction errors when blended with previous ones. The main idea is <b>to establish target outcomes for this upcoming model to minimize errors</b>.<br><br>\n        <mark><b>Gradient Boosting</b></mark> trains many models in <b>a gradual, additive and sequential manner</b>. The term gradient boosting emerged because every case‚Äôs target outcomes are based on the gradient‚Äôs error with regards to the predictions. Every model reduces prediction errors by taking a step in the correct direction.<br>\n        <center>\n            <img src=\"https://www.researchgate.net/publication/345327934/figure/fig3/AS:1022810793209856@1620868504478/Flow-chart-of-XGBoost.png\" alt=\"GB\" width=\"35%\"><br>\n            <i>üñº Boosting Algorithm by Rui Guo et al.</i>\n        </center>\n    </blockquote>\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- Gradient Boosting Parameters ---\nparameter_gb = {\n    \"algo__learning_rate\": [0.1, 0.3, 0.5]\n    , \"algo__n_estimators\": [2, 4, 6]\n    , \"algo__min_weight_fraction_leaf\": [0.1, 0.2, 0.5]\n}\n\n# --- Gradient Boosting Algorithm ---\nalgo_gb = GradientBoostingClassifier(loss=\"exponential\", random_state=2)\n\n# --- Applying Gradient Boosting ---\nacc_score_train_gb, acc_score_test_gb, best_score_gb = fit_ml_models(algo_gb, parameter_gb, \"Gradient Boosting\")","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:05:40.629395Z","iopub.execute_input":"2023-04-28T08:05:40.630963Z","iopub.status.idle":"2023-04-28T08:05:50.794496Z","shell.execute_reply.started":"2023-04-28T08:05:40.630884Z","shell.execute_reply":"2023-04-28T08:05:50.793129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div class=\"header2\">7.9 | AdaBoost</div>\n<div class=\"explain-box\">\n    <blockquote style=\"color: #000000;\">\n        <mark><b>AdaBoost</b></mark> also called <b>Adaptive Boosting</b> is a technique in Machine Learning used as an Ensemble Method. The most common algorithm used with AdaBoost is <b>decision trees with one level</b> that means with Decision trees with only 1 split. These trees are also called <mark><b>Decision Stumps</b></mark>. <b>AdaBoost builds a model and gives equal weights to all the data points</b>. It then assigns higher weights to points that are wrongly classified. Now, all the points which have higher weights are given more importance in the next model. It will keep training models until and unless a lowe error is received.\n    </blockquote>\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.081123,"end_time":"2022-12-02T09:14:46.945133","exception":false,"start_time":"2022-12-02T09:14:46.86401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- AdaBoost Parameters ---\nparameter_ab = {\n    \"algo__n_estimators\": [6, 7, 10]\n    , \"algo__learning_rate\": [0.2, 0.4, 0.8]\n}\n\n# --- AdaBoost Algorithm ---\nalgo_ab = AdaBoostClassifier(random_state=1)\n\n# --- Applying AdaBoost ---\nacc_score_train_ab, acc_score_test_ab, best_score_ab = fit_ml_models(algo_ab, parameter_ab, \"AdaBoost\")","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:05:50.796092Z","iopub.execute_input":"2023-04-28T08:05:50.797143Z","iopub.status.idle":"2023-04-28T08:06:01.256399Z","shell.execute_reply.started":"2023-04-28T08:05:50.79709Z","shell.execute_reply":"2023-04-28T08:06:01.254309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div class=\"header2\">7.10 | Model Comparison üëÄ</div>\n<div class=\"explain-box\">\n    After implementing and tuning 9 models, this section will <mark>compare all machine learning models accuracy and best score</mark>.\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"tags":[]}},{"cell_type":"code","source":"# --- Create Accuracy Comparison Table ---\ndf_compare = pd.DataFrame({'Model': ['Logistic Regression', 'K-Nearest Neighbour', 'Support Vector Machine', 'Gaussian NB',\n                                     'Decision Tree', 'Random Forest', 'Extra Tree Classifier', 'Gradient Boosting', 'AdaBoost'] \n                           , 'Accuracy Train': [acc_score_train_lr, acc_score_train_knn, acc_score_train_svc, acc_score_train_gnb,\n                                                acc_score_train_dt, acc_score_train_rf, acc_score_train_et, acc_score_train_gb, acc_score_train_ab]\n                           , 'Accuracy Test': [acc_score_test_lr, acc_score_test_knn, acc_score_test_svc, acc_score_test_gnb,\n                                               acc_score_test_dt, acc_score_test_rf, acc_score_test_et, acc_score_test_gb, acc_score_test_ab]\n                           , 'Best Score': [best_score_lr, best_score_knn, best_score_svc, best_score_gnb,best_score_dt, best_score_rf, \n                                            best_score_et, best_score_gb, best_score_ab]})\n\n# --- Create Comparison Table ---\nprint(clr.start+f\".:. Models Comparison .:.\"+clr.end)\nprint(clr.color+'*' * 26)\ndf_compare.sort_values(by='Best Score', ascending=False).style.apply(acc_train_vs_test, axis=1).hide_index()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-28T08:06:01.258465Z","iopub.execute_input":"2023-04-28T08:06:01.258948Z","iopub.status.idle":"2023-04-28T08:06:01.301434Z","shell.execute_reply.started":"2023-04-28T08:06:01.258902Z","shell.execute_reply":"2023-04-28T08:06:01.299075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"explain-box\">\n    From the results of the <mark>accuracy of the train and test</mark> above, <b>most models experienced overfitting or underfitting</b>. However, <b>several models have a good fit</b>, where the difference between train and test accuracy or vice versa is a little. These models are <b>random forest, AdaBoost, Gaussian Naive Bayes, and extra tree classifier</b>. As seen in the data frame above, of the four models, <mark>random forest and Gaussian NB have the highest accuracy compared to the other models</mark>. This is also supported by the ROC AUC curve figure for random forest and Gaussian NB, <mark>where the AUC value for both models is close to 1</mark>, which means that both models can predict well whether patients have heart disease. The <b>confusion matrix</b> shows that the prediction results between the actual target and the predicted target for the random forest and Gaussian NB models in each class in the test data are <mark>better</mark> than those of other models.<br><br>\n    Judging from the <mark>F1 scores</mark> of both models, both models do a very good job differentiating sick patients from those who are not (scores above <b>0.85</b>). If seen from the <mark>precision value for Gaussian NB, 93% of all the patients that the model predicted have heart disease. Whereas in the random forest precision value, only 88% out of all the patients that the model predicted have heart disease, slightly lower than the Gaussian NB precision value</mark>. At the <b>Gaussian NB recall value, this model only correctly predicts 81% of all heart disease patients. However, in the random forest recall value, this model can predict better than Gaussian NB, where 88% of patients are predicted to have heart disease out of all patients who do have heart disease</b>.<br><br>\n    Furthermore, in the <mark>learning curve</mark> between Gaussian NB and random forest, <mark>the learning curve for the random forest is more ideal</mark> than Gaussian NB. This is because <b>both training and validation scores of Gaussian NB stay too close together</b> (indicates low variance and high bias). This will more likely result in <mark>poor fit and especially poor generalization of the data (towards the data it has not seen before)</mark>. Whereas in a <b>random forest</b>, <mark>the validation score constantly improves as the number of training set sizes gets larger (notice the difference in the x-axis scale from the previous two curves)</mark>. Both the training and validation scores also converge to nearly similar values. This is a model that can generalize very well. From the analysis above, <mark>it can be concluded that the random forest model best predicts whether a person has heart disease</mark>.<br><br>\n    In the <mark>random forest feature importance plot</mark>, the <b>five following features to be the most important</b> are <mark>major vessels number (ca), fixed defect thalassemia (thal_2), ST depression induced by exercise relative to rest (oldpeak), reversable defect thalassemia (thal_3), and exercise-induced angina (exang)</mark>. <mark>Major vessel number</mark> might be significant since a lower number of major vessels can lead to a reduced blood supply to the heart muscle, which can result in ischemia (lack of oxygen and nutrients) and potentially lead to heart disease. For example, individuals with single-vessel disease have a greater risk of adverse cardiac events than those with multi-vessel disease. <mark>Fixed defects and reversible defects of thalassemia</mark> can affect the heart by reducing the number of red blood cells and oxygen delivery to the heart muscles, which can lead to the development of myocardial ischemia, coronary artery disease, and other forms of heart disease. Early detection and appropriate treatment of thalassemia can help prevent these complications and improve cardiovascular health.<br><br>\n    <mark>ST depression induced by exercise relative to rest</mark> can be important finding which can suggest underlying heart disease and may warrant further evaluation and management to reduce the risk of adverse cardiovascular events. Early detection and appropriate treatment of underlying heart disease can help prevent complications and improve cardiovascular health. Finally, <mark>exercise-induced angina</mark> is also important because it can suggest underlying CAD and may warrant further evaluation and management to reduce the risk of adverse cardiovascular events. Individuals with exercise-induced angina may need further testing, such as stress testing or coronary angiography, to assess the extent of their coronary artery disease and determine the most appropriate treatment options.\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.292259,"end_time":"2022-12-02T09:14:59.621924","exception":false,"start_time":"2022-12-02T09:14:59.329665","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# <div class=\"header1\">8. | Miscellaneous üß™</div>\n<div class=\"explain-box\">\n    This section focuses on <mark>creating a complete pipeline</mark>, starting from data processing to a machine learning pipeline, using the best model concluded in the previous section and <mark>exporting it to <code>joblib</code> and <code>pickle (.pkl)</code> files</mark>. Besides that, <mark>test dataset predicted results would also be exported</mark> along with actual results in CSV and JSON files. Moreover, this section will also <mark>make predictions on dummy data</mark> (data generated using Python functions) and <mark>export them to CSV and JSON files</mark>.\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"tags":[]}},{"cell_type":"markdown","source":"## <div class=\"header2\">8.1 | Creating Outputs üì§</div>\n<div class=\"explain-box\">\n    <mark>The complete pipeline will be exported in this section</mark>. The pipeline will be stored using the joblib library into <code>joblib</code> and <code>pickle (.pkl)</code> files. This section will also <mark>show the test data frame before exporting the predicted results and the actual results to the CSV and JSON files</mark>.\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{}},{"cell_type":"code","source":"# --- Complete Pipeline: Preprocessor & RF ---\nrf_pipeline = Pipeline([\n    ('preprocessor', preprocessor)\n    , ('algo', RandomForestClassifier(max_depth=3, random_state=99, n_jobs=-1))\n])\n\n# --- Save Complete Pipeline (joblib and pickle) ---\nfile_name = 'pipeline_heart_disease_random_forest_caesarmario'\nfor ext in ['joblib', 'pkl']:\n    joblib.dump(rf_pipeline, f'pipeline/{file_name}.{ext}')","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:06:01.304299Z","iopub.execute_input":"2023-04-28T08:06:01.305464Z","iopub.status.idle":"2023-04-28T08:06:01.323098Z","shell.execute_reply.started":"2023-04-28T08:06:01.305387Z","shell.execute_reply":"2023-04-28T08:06:01.320648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Dataframes to Create Test Output Dataframe ---\nrf_pipeline.fit(x_train, y_train)\ny_pred_rf = rf_pipeline.predict(x_test)\npred_target = pd.DataFrame(y_pred_rf, columns=['pred_target'])\n\nx_test_output = x_test.reset_index()\nactual_target = y_test.to_frame(name='actual_target').reset_index()\n\n# --- Combining and Creating Test Output Dataframe ---\ndf_test_output = pd.concat([x_test_output, actual_target, pred_target], axis=1).drop('index', axis=1)\n\n# --- Showing Sample Test Output Dataframe ---\nprint(clr.start+'.: Sample Test Dataframe :.'+clr.end)\nprint(clr.color+'*' * 28)\ndf_test_output.sample(n=10, random_state=0).style.apply(act_vs_pred, axis=1).hide_index()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-28T08:06:01.32504Z","iopub.execute_input":"2023-04-28T08:06:01.325462Z","iopub.status.idle":"2023-04-28T08:06:01.851976Z","shell.execute_reply.started":"2023-04-28T08:06:01.325424Z","shell.execute_reply":"2023-04-28T08:06:01.850247Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Export to CSV and JSON Files ---\noutput_name = 'test_data_heart_disease_caesarmario'\ndf_test_output.to_csv(f'test_data/{output_name}.csv', index=False, sep=',', encoding='utf-8')\ndf_test_output.to_json(f'test_data/{output_name}.json', orient='index')","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:06:01.853412Z","iopub.execute_input":"2023-04-28T08:06:01.853793Z","iopub.status.idle":"2023-04-28T08:06:01.869446Z","shell.execute_reply.started":"2023-04-28T08:06:01.853757Z","shell.execute_reply":"2023-04-28T08:06:01.867499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <div class=\"header2\">8.2 | Prediction Case üßê</div>\n<div class=\"explain-box\">\n    This second section will <mark>predict the dummy data</mark> generated using Python functions. Then, <mark>the prediction results will be exported as CSV and JSON files</mark>, along with dummy data.\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{}},{"cell_type":"code","source":"# --- Creating Prediction Case Dataframe (50 Rows) ---\ndf_pred_case = create_prediction_case(x_train, 50)\n\n# --- Showing Dataframe ---\nprint(clr.start+'.: Prediction Case Dataframe :.'+clr.end)\nprint(clr.color+'*' * 32)\ndf_pred_case.sample(n=6, random_state=24).style.background_gradient(cmap='Reds').hide_index()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-28T08:06:01.871211Z","iopub.execute_input":"2023-04-28T08:06:01.872226Z","iopub.status.idle":"2023-04-28T08:06:01.925539Z","shell.execute_reply.started":"2023-04-28T08:06:01.872159Z","shell.execute_reply":"2023-04-28T08:06:01.92383Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"explain-box\">\n    The above data frame is <mark>six samples from 50 dummy data</mark> generated using Python. Furthermore, using the best model in the next section, <b>predictions will be made on dummy data, displaying the prediction results in a data frame before exporting it to CSV and JSON files</b>.\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"papermill":{"duration":0.292259,"end_time":"2022-12-02T09:14:59.621924","exception":false,"start_time":"2022-12-02T09:14:59.329665","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# --- Creating Prediction using Best Model ---\ny_pred_case = rf_pipeline.predict(df_pred_case)\n\n# --- Combining Prediction Case Dataframe w/ Prediction ---\npred_case_target = pd.DataFrame(y_pred_case, columns=['pred_target'])\ndf_pred_case = pd.concat([df_pred_case, pred_case_target], axis=1)\n\n# --- Showing Final Dataframe ---\nprint(clr.start+'.: Final Prediction Case Dataframe :.'+clr.end)\nprint(clr.color+'*' * 38)\ndf_pred_case.sample(n=6, random_state=24).style.apply(coloring_target_col).hide_index()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-28T08:06:01.927586Z","iopub.execute_input":"2023-04-28T08:06:01.928419Z","iopub.status.idle":"2023-04-28T08:06:02.076626Z","shell.execute_reply.started":"2023-04-28T08:06:01.928371Z","shell.execute_reply":"2023-04-28T08:06:02.074701Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Export to CSV and JSON Files ---\npred_output_name = 'pred_case_heart_disease_caesarmario'\ndf_pred_case.to_csv(f'pred_case/{pred_output_name}.csv', index=False, sep=',', encoding='utf-8')\ndf_pred_case.to_json(f'pred_case/{pred_output_name}.json', orient='index')","metadata":{"execution":{"iopub.status.busy":"2023-04-28T08:06:02.078652Z","iopub.execute_input":"2023-04-28T08:06:02.07903Z","iopub.status.idle":"2023-04-28T08:06:02.088698Z","shell.execute_reply.started":"2023-04-28T08:06:02.078994Z","shell.execute_reply":"2023-04-28T08:06:02.087017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div class=\"header1\">9. | Conclusions and Future Improvements üßê</div>\n<div class=\"explain-box\">\n    From the results of dataset analysis and implementation of machine learning models in the previous section, <mark>it can be concluded as follows</mark>:\n    <blockquote style=\"color: #000000;\">\n        <ul>\n            <li><mark>Random forest is the best model</mark> out of 9 machine-learning models implemented in this notebook. This is because <b>this model fits well with train and test data</b>. In addition, <b>this model also performs better than other models when predicting the test data</b> (can be seen from the performance evaluation graph and classification report of each model).</li>\n            <li>Based on previous findings, <mark>medical workers can focus more on examining the five variables previously mentioned</mark>. This is because these five variables most influence whether a patient has heart disease.</li>\n            <li><mark>The prediction results on test data, dummy data, and the complete machine learning pipeline have been successfully exported</mark> for other purposes. In addition, data exploration has also been successfully carried out using the <code>ydata-profiling</code>, <code>seaborn</code>, and <code>matplotlib</code> libraries.</li>\n            <li><mark>Several improvements can be implemented in the following research/notebook</mark>. For example, by carrying out A/B Testing on patients with the same major vessel number in one group. Another example is performing advanced hyperparameter tuning experiments to obtain higher accuracy (~90%).</li>\n        </ul>\n    </blockquote>\n</div>\n<!-- Hello world üëã. Thank you so much for downloading/forking my codes/works. If you like my works, please support me by giving upvotes and comments on my Kaggle profile (https://www.kaggle.com/caesarmario/). Thank you so much and have a great day üòÜüëç. More about me: https://linktr.ee/caesarmario_ -->","metadata":{"tags":[]}},{"cell_type":"markdown","source":"# <div class=\"header1\">10. | References üîó</div>\n<div class=\"references\">\n    <ul><u>Kaggle Notebook üìö</u>\n        <li><a style=\"color: #3D5A80\" href=\"https://www.kaggle.com/vivek468/what-visualizations-should-you-use\">What Visualizations Should You Use? by Vivek Chowdhury</a></li>\n        <li><a style=\"color: #3D5A80\" href=\"https://www.kaggle.com/code/sonalisingh1411/eda-on-train-test-dataset-price-prediction\">EDA On Train & Test Dataset+üè° Priceüí∏Predictionü§î by Sonali Singh</a></li>\n        <li><a style=\"color: #3D5A80\" href=\"https://www.kaggle.com/cdabakoglu/heart-disease-classifications-machine-learning\">Heart Disease - Classifications (Machine Learning) by Caner Dabakoglu</a></li>\n        <li><a style=\"color: #3D5A80\" href=\"https://www.kaggle.com/code/asimislam/heart-disease-uci-eda-and-ml-w-lr\">Heart Disease UCI - EDA and ML w/LR by Asim Islam</a></li>\n        <li><a style=\"color: #3D5A80\" href=\"https://www.kaggle.com/code/kellibelcher/heart-disease-predictions-with-shapley\">Heart Disease Predictions with Shapley by Kelli Belcher</a></li>\n    </ul>\n    <ul><b><u>Online Articles üåè</u></b>\n        <li><a style=\"color: #3D5A80\" href=\"https://www.simplilearn.com/tutorials/machine-learning-tutorial/logistic-regression-in-python\">An Introduction to Logistic Regression in Python by Simplilearn</a></li>\n        <li><a style=\"color: #3D5A80\" href=\"https://learn.g2.com/k-nearest-neighbor\">What Is K-Nearest Neighbor? An ML Algorithm to Classify Data by Amal Joby</a></li>\n        <li><a style=\"color: #3D5A80\" href=\"https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm\">Support Vector Machine Algorithm by Javatpoint</a></li>\n        <li><a style=\"color: #3D5A80\" href=\"https://iq.opengenus.org/gaussian-naive-bayes/\">Gaussian Naive Bayes by OpenGenus</a></li>\n        <li><a style=\"color: #3D5A80\" href=\"https://www.javatpoint.com/machine-learning-decision-tree-classification-algorithm\">Decision Tree Classification Algorithm by Javatpoint</a></li>\n        <li><a style=\"color: #3D5A80\" href=\"https://www.analyticsvidhya.com/blog/2020/05/decision-tree-vs-random-forest-algorithm/\">Decision Tree vs. Random Forest ‚Äì Which Algorithm Should you Use? by Abhishek Sharma</a></li>\n        <li><a style=\"color: #3D5A80\" href=\"https://towardsdatascience.com/understanding-random-forest-58381e0602d2\">Understanding Random Forest by Tony Yiu</a></li>\n        <li><a style=\"color: #3D5A80\" href=\"https://datascience.eu/machine-learning/gradient-boosting-what-you-need-to-know/\">Gradient Boosting ‚Äì What You Need to Know by Data Science.EU</a></li>\n        <li><a style=\"color: #3D5A80\" href=\"https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab\">Understanding Gradient Boosting Machines by Harshdeep Singh</a></li>\n        <li><a style=\"color: #3D5A80\" href=\"https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/\">AdaBoost Algorithm ‚Äì A Complete Guide for Beginners by Anshul Saini</a></li>\n        <li><a style=\"color: #3D5A80\" href=\"https://www.geeksforgeeks.org/ml-extra-tree-classifier-for-feature-selection/\">ML | Extra Tree Classifier for Feature Selection by GeeksforGeeks</a></li>\n    </ul>\n</div>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<center>\n    <span class=\"thanks\">.: Thanks for Viewing :.</span><br><br>\n    <span class=\"thanks-explain\">üìå Like this notebook? You can support me by giving <mark><b>upvote</b></mark> üòÜüëçüîº</span><br>\n    <span class=\"three-dots2\">...</span><br><br>\n    <span class=\"thanks-explain\">Similar project on <b><u>GitHub</u></b>:</span><br><br>\n    <span class=\"promotion\">‚ñ∏‚ñ∏ Disease Prediction using <b>SAS Studio</b> <a href=\"https://github.com/caesarmario/heart-disease-prediction-with-logistic-regression-SAS-studio\">here</a> ‚óÇ‚óÇ</span><br><br>\n    <span class=\"thanks-watermark\"><u>Support me!</u></span><br>\n    <span class=\"ko-fi\">\n        <a href='https://ko-fi.com/D1D3JU963' target='_blank'><img src='https://ko-fi.com/img/githubbutton_sm.svg' alt='Support me on Ko-fi Button'/></a>\n    </span><br>\n    <span class=\"thanks-watermark\"><u>Follow me in other platform</u></span><br>\n    <div align=\"center\" class=\"social-media\">\n        <ul>\n            <li><a href=\"https://www.kaggle.com/caesarmario\"><img src=\"https://i.imgur.com/K6QyzaJ.png\"></a></li>\n            <li><a href=\"https://public.tableau.com/app/profile/caesarmario\"><img src=\"https://i.imgur.com/JVxVkeQ.png\"></a></li>\n            <li><a href=\"https://github.com/caesarmario\"><img src=\"https://i.imgur.com/Orp40Ys.png\"></a></li>\n            <li><a href=\"https://caesarmario.medium.com/\"><img src=\"https://i.imgur.com/6TrHyu0.png\"></a></li>\n            <li><a href=\"https://www.linkedin.com/in/caesarmario\"><img src=\"https://i.imgur.com/vVYd0aI.png\"></a></li>\n        </ul>\n    </div>\n    <img src=\"https://i.imgur.com/Xy0J9G0.png\" width=65% alt=\"WM\">\n</center>","metadata":{"tags":[]}}]}